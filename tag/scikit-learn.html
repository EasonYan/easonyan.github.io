<!DOCTYPE html>
<html lang="zn">
<head>
        <meta charset="utf-8" />
        <title>eason_blog - scikit-learn</title>
        <link rel="stylesheet" href="/theme/css/main.css" />

        <!--[if IE]>
            <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
        <![endif]-->
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="/">eason_blog </a></h1>
                <nav><ul>
                    <li><a href="/category/bazinga.html">bazinga</a></li>
                    <li><a href="/category/mechine-learning.html">mechine learning</a></li>
                </ul></nav>
        </header><!-- /#banner -->

            <aside id="featured" class="body">
                <article>
                    <h1 class="entry-title"><a href="/pa-keng-zhi-lu-wen-ben-ju-lei-zong-jie-3.html">爬坑之路：文本聚类总结(3)</a></h1>
<footer class="post-info">
        <abbr class="published" title="2014-08-05T00:00:00">
                周二 05 八月 2014
        </abbr>

        <address class="vcard author">
                By <a class="url fn" href="/author/eason.html">eason</a>
        </address>
<p>In <a href="/category/mechine-learning.html">mechine learning</a>. </p>
<p>tags: <a href="/tag/python.html">python</a><a href="/tag/scikit-learn.html">scikit-learn</a><a href="/tag/.html"></a></p>
</footer><!-- /.post-info --><h6></h6>
<p>断续有两个星期的，扎堆在文本聚类上，今天是来画上个顿号的。初衷就是想来做个练手用的，选几个无监督学习的算法，套下文本聚类，到此也大概达到了一半目的。这里用了k-mean和bisecting k-means,没有做太多的优化，也不打算继续去套用其他聚类算法。因为这样下去的学习收获就和付出时间不成正比了。毕竟并不是想一头钻进聚类里面。好聚好散，外面的世界多精彩。大方向的过程如下。</p>
<h3>1. 选语料，这里用的是数据堂的新闻语料，<a href="http://www.datatang.com/data/11971/">点我</a>分类的算好，但是由于文档大小不一，个别内容太少的文档还是对聚类结果产生了不小的影响</h3>
<h3>2. 采取语料，进行分词，这里采用了<em>jieba</em>分词库，还算挺不错的。</h3>
<h3>3. 预处理，去除停用词，停用词表用的比较杂，主要还是<a href="http://www.datatang.com/datares/go.aspx?dataid=616671">再点我</a>和哈工大停用词表几个词表的融合。</h3>
<h3>4. 特征提取及降维。就此案例，近3000个文档，去停用词后的分词结果为72万词，独立词汇为7万个，采用基本的DF去降维数。所以借助了scikit-learn 去进行文档频率的计算。几次下来发现，采用<em>DF&gt;50</em>的准确率要比采用<em>DF&gt;100</em>高出不少，大概有提高<em>10%</em>的准确率，但代价就是维数因此翻了好几倍，计算量大。</h3>
<h3>5. 之后就是套用聚类算法了。上一篇中讲过K-Mean聚类效果像陀便便，惨不忍睹，后来debug发现其实是我笨得像陀屎，居然将余弦值越接近1向量夹角才越靠近这个概念理解相反了。哎，傻逼了。具体的K-mean算法也不贴了，可具体参考我上一篇给出的链接，再根据自己实际需求更改。算法其实不难，难的是对<em>迭代终止条件</em>的设立。爬的最深的一个坑就是，按照每个文档与他们中心的余弦值之和作为聚类准则函数的话，准则函数并不是收敛的！反而是递增的！这个完全无法理解，也卡在这里很久，后来就直接用迭代次数的方法来终止迭代，在这个案例中迭代个5,6次，也就也就大概差不多了。</h3>
<h3>6. 聚类的效果:k-mean的聚类效果不稳定，可能是随机采用中心的原因，另外一些离群点（像非常稀疏的，过滤完后只有一两个词的文档）也没有去处理。总结是在计算机，经济，政治，艺术这几大类的聚类结果比较纯净，准确率很高，90%+。</h3>
<h3>7. bisecting k-means的聚类中迭代，选簇用的是最大sse即准则函数，这种选法比选取最大个数的簇要好得多，但可能实际应用的话要两者都结合起来。比较懒，也不去做了（呵呵，我不会难道也要告诉你么），bisecting k-means的聚类效果也是挺不稳定，但在特定的几个类别，如计算机，政治，经济，教育等聚类效果也都挺不错，所以我估计会不会是语料的选取。可能本身语料的质量也不是很高呀。这也是我为什么不进行优化的原因，在知识水平不够的前提下，根本无法判定当下的误差是由哪个大问题造成的，茫茫然扎进去，得不偿失。</h3>
<h3>100. 巴拉巴拉总结的话：虽然说是练手的，但几次实验下来聚类效果有5,60%。也是觉得挺OK了，初步练手的目的也算达到。虽说没多大技术含量，也行呗。好歹端正了以往的些许观念。世界是无穷尽的，并不是你做一样东西什么都要从零开始，可以发明工具，那就勇敢的去。做不到，就要利用工具，为你节省时间和精力，计算的东西交给计算机。本末倒置是很可怕，分析和解决问题，才是大头。年轻的时候，总还是会有很多狭隘的观念呀，该走的坑，一个都不会少。</h3>                </article>
            </aside><!-- /#featured -->
                <section id="content" class="body">
                    <h1>Other articles</h1>
                    <hr />
                    <ol id="posts-list" class="hfeed">

            <li><article class="hentry">
                <header>
                    <h1><a href="/pa-keng-zhi-lu-python-kmeanswen-ben-ju-lei-2.html" rel="bookmark"
                           title="Permalink to 爬坑之路：python KMeans文本聚类(2)">爬坑之路：python KMeans文本聚类(2)</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2014-08-01T00:00:00">
                周五 01 八月 2014
        </abbr>

        <address class="vcard author">
                By <a class="url fn" href="/author/eason.html">eason</a>
        </address>
<p>In <a href="/category/mechine-learning.html">mechine learning</a>. </p>
<p>tags: <a href="/tag/python.html">python</a><a href="/tag/scikit-learn.html">scikit-learn</a><a href="/tag/.html"></a></p>
</footer><!-- /.post-info -->                <h6></h6>
<p>折腾了很多天的文本聚类，今天终于以失败告终，不到10%的准确率。稍微总结下。</p>
<h2>1.  python实现，采用最基本的KMeans聚类算法。</h2>
<h2>2. 文档的特征值的选取，DF(document frequency)，采用DF值大于80的，即关键词至少出现在不同的80个文档中，方选取为文档的特征值。如此，文档向量维数为1000左右。</h2>
<h2>3. TF-IDF值借助python库scikit-learn计算，每个文档表示为scipy.sparse.csr_matrix格式的稀疏矩阵。如是下来，稀疏矩阵的四则运算都可轻易得到，同时减少运算量，提高聚类速度。</h2>
<h2>4. KMeans算法的实现大框架参考<a href="http://sobuhu.com/ml/2012/11/25/kmeans-python.html">click_me</a>。根据需求，使用<em>余弦相似度</em>计算文本距离，cost_funciton也为质心和文本向量的余弦相似度之和。</h2>
<h2>5. 大体思路如下：</h2>
<div class="highlight"><pre><span class="o">*</span>  <span class="err">随机选取初始质点</span>
<span class="o">*</span>  <span class="err">对每个文档向量，分配到最近的质点</span><span class="p">.</span>
<span class="o">*</span>  <span class="err">重新计算新的质点，计算</span><span class="n">cost_funciton</span><span class="p">.(</span><span class="n">sse</span><span class="p">)</span>
<span class="o">*</span>  <span class="err">不停迭代，直至</span><span class="n">cost_funciton</span><span class="err">收敛或接近一个阈值。迭代停止。</span>
</pre></div>


<h2>6 ...</h2>
                <a class="readmore" href="/pa-keng-zhi-lu-python-kmeanswen-ben-ju-lei-2.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/pa-keng-zhi-lu-pythonti-qu-wen-ben-te-zheng-ci-1.html" rel="bookmark"
                           title="Permalink to 爬坑之路：python提取文本特征词(1)">爬坑之路：python提取文本特征词(1)</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2014-07-26T00:00:00">
                周六 26 七月 2014
        </abbr>

        <address class="vcard author">
                By <a class="url fn" href="/author/eason.html">eason</a>
        </address>
<p>In <a href="/category/mechine-learning.html">mechine learning</a>. </p>
<p>tags: <a href="/tag/python.html">python</a><a href="/tag/scikit-learn.html">scikit-learn</a><a href="/tag/.html"></a></p>
</footer><!-- /.post-info -->                <h6></h6>
<ol>
<li>
<p>连日台风，甚是爽快。不快的是倒在文本分析的第一步上。想来是菜鸟的必由之路。用来练手的是新闻的聚类，3000个文档，用jieba分词，去完stop_words后有72万个词汇，单一词汇有7万个，这7万个词当然不能全部作为文档的特征值。慎重考虑后，一方面出于原理简单，实现容易，采用了DF（document frequency）来初步筛选文本关键词。</p>
</li>
<li>
<p>python实现，最开始傻逼呵呵的将七万个词建立一个字典，遍历词典，对每个关键词遍历3000个文档，每个文档初步去完停用词后是个列表，判断关键词是否在文档列表中，在则字典值+1</p>
</li>
</ol>
<h2>python</h2>
<div class="highlight"><pre><span class="k">for</span> <span class="n">word</span> <span class="n">in</span> <span class="n">word_dict</span><span class="o">:</span>
    <span class="k">for</span> <span class="n">docu</span> <span class="n">in</span> <span class="n">documents</span><span class="o">:</span>
        <span class="k">if</span> <span class="n">word</span> <span class="n">in</span> <span class="n">docu</span><span class="o">:</span>
            <span class="n">word_dict</span><span class="p">[</span><span class="n">word</span><span class="p">]</span><span class="o">+=</span><span class="mi">1</span>
</pre></div>


<ol>
<li>
<p>毫无疑问，这种做法在python里面绝逼是种撕逼行为啊，用脚趾头算一算，7万<em>3千</em>文档词汇，复杂度是O（KMN)，每份文档有100个词的话 ...</p></li></ol>
                <a class="readmore" href="/pa-keng-zhi-lu-pythonti-qu-wen-ben-te-zheng-ci-1.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/scikit-learnzhong-guan-yu-tf-idfde-ji-suan.html" rel="bookmark"
                           title="Permalink to scikit-learn中关于TF-IDF的计算">scikit-learn中关于TF-IDF的计算</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2014-07-20T00:00:00">
                周日 20 七月 2014
        </abbr>

        <address class="vcard author">
                By <a class="url fn" href="/author/eason.html">eason</a>
        </address>
<p>In <a href="/category/mechine-learning.html">mechine learning</a>. </p>
<p>tags: <a href="/tag/python.html">python</a><a href="/tag/scikit-learn.html">scikit-learn</a></p>
</footer><!-- /.post-info -->                <h6></h6>
<p>TF-IDF是文本特征向量的一种表达形式，可用于文本聚类和分类。谈及python的mechine learning,难以避免会涉及第三方的库sickit-learn。方便好用，接口多。稍嫌麻烦是其安装过程，pip安装报错，easy_install也莫名奇妙的错误，windows下简直不能忍受。后来一怒之下，上了<a href="http://sourceforge.net/projects/scipy/files/scipy/0.14.0/"><em>sorcepage</em></a>下载可执行文件。exe果真方便啊。</p>
<p>TF-IDF在scikit-learn中的<em>sklearn.feature_extraction.text</em>给出，那是如何计算呢。一般来讲，TF(term frequency)词在某一文档中出现的频率，IDF(inverse document frequency，IDF),由总文件数目除以包含该词语之文件的数目，再将得到的商取对数。但scikit-learn中的TfidfTransformer类提供了对TF-IDF值的优化，会通过相应参数改变TF-IDF值，其中有“norm”-&gt;是否进行向量规范化，“smooth_idf”-&gt;则会对文档总数、出现次数+1，对数完后再+1。具体stackoverflow上有篇帖子讲得很清楚，源代码也有贴出。<a href="http://stackoverflow.com/questions/24032485/difference-in-values-of-tf-idf-matrix-using-scikit-learn-and-hand-calculation">点此</a>。</p>
<p>至于特征向量为什么要用TF-IDF，IDF取对数而TF不取 ...</p>
                <a class="readmore" href="/scikit-learnzhong-guan-yu-tf-idfde-ji-suan.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>
            </ol><!-- /#posts-list -->
<p class="paginator">
    Page 1 / 1
</p>
            </section><!-- /#content -->
        <section id="extras" class="body">
                <div class="blogroll">
                        <h2>blogroll</h2>
                        <ul>
                            <li><a href="http://getpelican.com/">Pelican</a></li>
                            <li><a href="http://python.org/">Python.org</a></li>
                            <li><a href="http://jinja.pocoo.org/">Jinja2</a></li>
                        </ul>
                </div><!-- /.blogroll -->
                <div class="social">
                        <h2>social</h2>
                        <ul>

                            <li><a href="#">You can add links in your config file</a></li>
                            <li><a href="#">Another social link</a></li>
                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="http://getpelican.com/">Pelican</a>, which takes great advantage of <a href="http://python.org">Python</a>.
                </address><!-- /#about -->

                <p>The theme is by <a href="http://coding.smashingmagazine.com/2009/08/04/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
        </footer><!-- /#contentinfo -->

</body>
</html>