<!DOCTYPE html>
<html lang="zn">
<head>
        <meta charset="utf-8" />
        <title>eason_blog - eason</title>
        <link rel="stylesheet" href="/theme/css/main.css" />

        <!--[if IE]>
            <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
        <![endif]-->
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="/">eason_blog </a></h1>
                <nav><ul>
                    <li><a href="/category/bazinga.html">bazinga</a></li>
                    <li><a href="/category/mechine-learning.html">mechine learning</a></li>
                </ul></nav>
        </header><!-- /#banner -->

            <aside id="featured" class="body">
                <article>
                    <h1 class="entry-title"><a href="/pa-keng-zhi-lu-python-kmeanswen-ben-ju-lei-2.html">爬坑之路：python KMeans文本聚类(2)</a></h1>
<footer class="post-info">
        <abbr class="published" title="2014-08-01T00:00:00">
                周五 01 八月 2014
        </abbr>

        <address class="vcard author">
                By <a class="url fn" href="/author/eason.html">eason</a>
        </address>
<p>In <a href="/category/mechine-learning.html">mechine learning</a>. </p>
<p>tags: <a href="/tag/python.html">python</a><a href="/tag/scikit-learn.html">scikit-learn</a><a href="/tag/.html"></a></p>
</footer><!-- /.post-info --><h6></h6>
<p>折腾了很多天的文本聚类，今天终于以失败告终，不到10%的准确率。稍微总结下。</p>
<h2>1.  python实现，采用最基本的KMeans聚类算法。</h2>
<h2>2. 文档的特征值的选取，DF(document frequency)，采用DF值大于80的，即关键词至少出现在不同的80个文档中，方选取为文档的特征值。如此，文档向量维数为1000左右。</h2>
<h2>3. TF-IDF值借助python库scikit-learn计算，每个文档表示为scipy.sparse.csr_matrix格式的稀疏矩阵。如是下来，稀疏矩阵的四则运算都可轻易得到，同时减少运算量，提高聚类速度。</h2>
<h2>4. KMeans算法的实现大框架参考<a href="http://sobuhu.com/ml/2012/11/25/kmeans-python.html">click_me</a>。根据需求，使用<em>余弦相似度</em>计算文本距离，cost_funciton也为质心和文本向量的余弦相似度之和。</h2>
<h2>5. 大体思路如下：</h2>
<div class="highlight"><pre><span class="o">*</span>  <span class="err">随机选取初始质点</span>
<span class="o">*</span>  <span class="err">对每个文档向量，分配到最近的质点</span><span class="p">.</span>
<span class="o">*</span>  <span class="err">重新计算新的质点，计算</span><span class="n">cost_funciton</span><span class="p">.(</span><span class="n">sse</span><span class="p">)</span>
<span class="o">*</span>  <span class="err">不停迭代，直至</span><span class="n">cost_funciton</span><span class="err">收敛或接近一个阈值。迭代停止。</span>
</pre></div>


<h2>6. 为了减少计算的重复性，每个文档为一个类，类中包含稀疏矩阵向量，和每个向量的大小。这样计算预先相似度时直接调用这个属性即可。</h2>
<h2>7. 聚类结果，准确率奇低，迭代一次之后就停止了，我这里设置阈值为10，即只要sse损耗比上一次聚类相差不到10，迭代停止。聚类效果这么差，KMeans 算法在无大出错前提下，就只能是此Kmeans 算法并不适合于做文本聚类，或者是文本特征提取的不好，权重设置的不准确之类。计算速度其实还好，3000个文档，1000矩阵运算，迭代一次的时间为10秒钟左右，具体失败细节，没错，国家情报人员正在努力排查中。哎。</h2>                </article>
            </aside><!-- /#featured -->
                <section id="content" class="body">
                    <h1>Other articles</h1>
                    <hr />
                    <ol id="posts-list" class="hfeed">

            <li><article class="hentry">
                <header>
                    <h1><a href="/pa-keng-zhi-lu-pythonti-qu-wen-ben-te-zheng-ci-1.html" rel="bookmark"
                           title="Permalink to 爬坑之路：python提取文本特征词(1)">爬坑之路：python提取文本特征词(1)</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2014-07-26T00:00:00">
                周六 26 七月 2014
        </abbr>

        <address class="vcard author">
                By <a class="url fn" href="/author/eason.html">eason</a>
        </address>
<p>In <a href="/category/mechine-learning.html">mechine learning</a>. </p>
<p>tags: <a href="/tag/python.html">python</a><a href="/tag/scikit-learn.html">scikit-learn</a><a href="/tag/.html"></a></p>
</footer><!-- /.post-info -->                <h6></h6>
<ol>
<li>
<p>连日台风，甚是爽快。不快的是倒在文本分析的第一步上。想来是菜鸟的必由之路。用来练手的是新闻的聚类，3000个文档，用jieba分词，去完stop_words后有72万个词汇，单一词汇有7万个，这7万个词当然不能全部作为文档的特征值。慎重考虑后，一方面出于原理简单，实现容易，采用了DF（document frequency）来初步筛选文本关键词。</p>
</li>
<li>
<p>python实现，最开始傻逼呵呵的将七万个词建立一个字典，遍历词典，对每个关键词遍历3000个文档，每个文档初步去完停用词后是个列表，判断关键词是否在文档列表中，在则字典值+1</p>
</li>
</ol>
<h2>python</h2>
<div class="highlight"><pre><span class="k">for</span> <span class="n">word</span> <span class="n">in</span> <span class="n">word_dict</span><span class="o">:</span>
    <span class="k">for</span> <span class="n">docu</span> <span class="n">in</span> <span class="n">documents</span><span class="o">:</span>
        <span class="k">if</span> <span class="n">word</span> <span class="n">in</span> <span class="n">docu</span><span class="o">:</span>
            <span class="n">word_dict</span><span class="p">[</span><span class="n">word</span><span class="p">]</span><span class="o">+=</span><span class="mi">1</span>
</pre></div>


<ol>
<li>
<p>毫无疑问，这种做法在python里面绝逼是种撕逼行为啊，用脚趾头算一算，7万<em>3千</em>文档词汇，复杂度是O（KMN)，每份文档有100个词的话 ...</p></li></ol>
                <a class="readmore" href="/pa-keng-zhi-lu-pythonti-qu-wen-ben-te-zheng-ci-1.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/da-jian-pelicanyu-githubde-bo-ke.html" rel="bookmark"
                           title="Permalink to 搭建pelican与github的博客">搭建pelican与github的博客</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2014-07-20T00:00:00">
                周日 20 七月 2014
        </abbr>

        <address class="vcard author">
                By <a class="url fn" href="/author/eason.html">eason</a>
        </address>
<p>In <a href="/category/bazinga.html">bazinga</a>. </p>
<p>tags: <a href="/tag/python.html">python</a></p>
</footer><!-- /.post-info -->                <p>废话巴拉巴拉开始
这学期中开始想要折腾一个博客，学的是python，pelican当然是首选。
学习过程中爬的坑还是不少，借助网络的资源，大抵也都能够解决。
搜索pelican+github为关键字，网络上也有蛮多教程的，由于是在window上搭建，没有make命令，安装make命令后又有各种乱起八糟的错误，索性就直接用<em>pelican content - o output -s pelicanconf.py</em>来生成html文件，也都挺好用，就是命令行长了一点。</p>
<p>很深刻的感触的是，人真的要学会分享，如何能够在有限的时间学到足够广袤的知识呢，互联网开源精神值得敬重。从小到大，我们所谓的学习，大部分都是在学习人类几千年来传承下来的经验和工具，创造永远都不是凭空的，即使无法创新，单单学习这些东西，都够你受用了。</p>
<p>也恰好暑假会开始折腾下小小的文本分析，有坑，与君共勉。</p>
                <a class="readmore" href="/da-jian-pelicanyu-githubde-bo-ke.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/scikit-learnzhong-guan-yu-tf-idfde-ji-suan.html" rel="bookmark"
                           title="Permalink to scikit-learn中关于TF-IDF的计算">scikit-learn中关于TF-IDF的计算</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2014-07-20T00:00:00">
                周日 20 七月 2014
        </abbr>

        <address class="vcard author">
                By <a class="url fn" href="/author/eason.html">eason</a>
        </address>
<p>In <a href="/category/mechine-learning.html">mechine learning</a>. </p>
<p>tags: <a href="/tag/python.html">python</a><a href="/tag/scikit-learn.html">scikit-learn</a></p>
</footer><!-- /.post-info -->                <h6></h6>
<p>TF-IDF是文本特征向量的一种表达形式，可用于文本聚类和分类。谈及python的mechine learning,难以避免会涉及第三方的库sickit-learn。方便好用，接口多。稍嫌麻烦是其安装过程，pip安装报错，easy_install也莫名奇妙的错误，windows下简直不能忍受。后来一怒之下，上了<a href="http://sourceforge.net/projects/scipy/files/scipy/0.14.0/"><em>sorcepage</em></a>下载可执行文件。exe果真方便啊。</p>
<p>TF-IDF在scikit-learn中的<em>sklearn.feature_extraction.text</em>给出，那是如何计算呢。一般来讲，TF(term frequency)词在某一文档中出现的频率，IDF(inverse document frequency，IDF),由总文件数目除以包含该词语之文件的数目，再将得到的商取对数。但scikit-learn中的TfidfTransformer类提供了对TF-IDF值的优化，会通过相应参数改变TF-IDF值，其中有“norm”-&gt;是否进行向量规范化，“smooth_idf”-&gt;则会对文档总数、出现次数+1，对数完后再+1。具体stackoverflow上有篇帖子讲得很清楚，源代码也有贴出。<a href="http://stackoverflow.com/questions/24032485/difference-in-values-of-tf-idf-matrix-using-scikit-learn-and-hand-calculation">点此</a>。</p>
<p>至于特征向量为什么要用TF-IDF，IDF取对数而TF不取 ...</p>
                <a class="readmore" href="/scikit-learnzhong-guan-yu-tf-idfde-ji-suan.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>
            </ol><!-- /#posts-list -->
<p class="paginator">
    Page 1 / 1
</p>
            </section><!-- /#content -->
        <section id="extras" class="body">
                <div class="blogroll">
                        <h2>blogroll</h2>
                        <ul>
                            <li><a href="http://getpelican.com/">Pelican</a></li>
                            <li><a href="http://python.org/">Python.org</a></li>
                            <li><a href="http://jinja.pocoo.org/">Jinja2</a></li>
                        </ul>
                </div><!-- /.blogroll -->
                <div class="social">
                        <h2>social</h2>
                        <ul>

                            <li><a href="#">You can add links in your config file</a></li>
                            <li><a href="#">Another social link</a></li>
                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="http://getpelican.com/">Pelican</a>, which takes great advantage of <a href="http://python.org">Python</a>.
                </address><!-- /#about -->

                <p>The theme is by <a href="http://coding.smashingmagazine.com/2009/08/04/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
        </footer><!-- /#contentinfo -->

</body>
</html>